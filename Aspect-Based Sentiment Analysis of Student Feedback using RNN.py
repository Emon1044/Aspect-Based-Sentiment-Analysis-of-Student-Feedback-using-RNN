# -*- coding: utf-8 -*-
"""FinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1__c4-nlSkDzFDamov6PfY4bHFV9UfB9V
"""

import os, sys, json, time, random, shutil

DATA_PATH = "/content/ReadyToTrain_data_2col_with_subjectivity_final.tsv"
OUT_DIR = "/content/project_artifacts"
os.makedirs(OUT_DIR, exist_ok=True)

NROWS = 300000
FORCE_TRAIN = False
TARGET_ACC_STEP1 = 0.80
TARGET_ACC_STEP2 = 0.85

print(f"Data Path: {DATA_PATH}")
print(f"Output Directory: {OUT_DIR}")
print(f"Max Rows Loaded: {NROWS}")
print("---")

import numpy as np
import tensorflow as tf

def set_seeds(seed=42):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)

set_seeds(42)

import platform
import psutil
import multiprocessing

def get_system_specs():
    print("="*45)
    print("ENVIRONMENT & HARDWARE SPECS")
    print("="*45)
    print(f"Python Version: {sys.version.split()[0]}")
    print(f"TensorFlow Version: {tf.__version__}")
    print(f"OS: {platform.system()} {platform.release()}")

    print(f"CPU Count: {multiprocessing.cpu_count()}")
    ram_info = psutil.virtual_memory()
    print(f"Total RAM: {ram_info.total / (1024**3):.2f} GB")

    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        print(f"GPU Detected: {len(gpus)} device(s)")
        try:
            gpu_name = !nvidia-smi --query-gpu=name --format=csv,noheader
            print(f"GPU Name: {gpu_name[0]}")
            gpu_mem = !nvidia-smi --query-gpu=memory.total --format=csv,noheader
            print(f"GPU Memory: {gpu_mem[0]}")
        except Exception:
             print("(Specific GPU details unavailable via shell command)")
    else:
        print("GPU: None (Training will be on CPU)")
    print("="*45)

get_system_specs()

import pandas as pd

df_raw = pd.read_csv(DATA_PATH, sep='\t', header=0, encoding='utf-8', nrows=NROWS)
print("Loaded raw data shape:", df_raw.shape)

def infer_columns(df):
    text_col=None; label_col=None
    text_candidates = ['text','content','review','studentcomments','comment','comments','studentcomment']
    for c in df.columns:
        if c.lower() in text_candidates:
            text_col=c; break
    label_candidates = ['label','target','class','sentiment','subjectivity','sentiment_label']
    for c in df.columns:
        if c.lower() in label_candidates:
            label_col=c; break
    if text_col is None: text_col=df.columns[0]
    if label_col is None: label_col=df.columns[1] if len(df.columns)>1 else df.columns[-1]
    return text_col, label_col

TEXT_COL, LABEL_COL = infer_columns(df_raw)
print("Inferred TEXT_COL:", TEXT_COL, "LABEL_COL:", LABEL_COL)

df = df_raw[[TEXT_COL, LABEL_COL]].dropna().reset_index(drop=True)
counts = df[LABEL_COL].value_counts()
print("\nCleaned data shape:", df.shape)
print("Original class counts:\n", counts)

import matplotlib.pyplot as plt
import seaborn as sns

counts = df[LABEL_COL].value_counts()
plt.figure(figsize=(6,4))
sns.barplot(x=counts.values, y=counts.index, palette='crest')
plt.title("Original Class Distribution")
plt.show()

df['text_len'] = df[TEXT_COL].astype(str).apply(lambda s: len(str(s).split()))
print(f"Max text length (words): {df['text_len'].max()}")

from wordcloud import WordCloud
from nltk.corpus import stopwords
import nltk

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

STOPWORDS = set(stopwords.words('english'))

def plot_word_cloud(text_data, title):
    all_text = ' '.join([str(t) for t in text_data if pd.notna(t)])

    if not all_text:
        print(f"Skipping WordCloud for '{title}' - no valid text data.")
        return

    wc = WordCloud(width=800, height=400, background_color='white',
                   max_words=100, stopwords=STOPWORDS, colormap='magma').generate(all_text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"Word Cloud - {title}", fontsize=16)
    plt.show()

print("\nGenerating WordClouds by Class...")
for label in df[LABEL_COL].unique():
    subset = df[df[LABEL_COL] == label][TEXT_COL]
    plot_word_cloud(subset, f"Class: {label}")

plt.figure(figsize=(10, 5))
sns.histplot(df, x='text_len', hue=LABEL_COL, kde=True, bins=50, palette='viridis', multiple='stack')
plt.title("Text Length Distribution by Class")
plt.xlabel("Review Word Count")
plt.show()

from sklearn.utils import resample

print("\nHandling Class Imbalance...")
majority_size = df[LABEL_COL].value_counts().max()
print(f"Majority class size: {majority_size}")

df_upsampled = []
for label in df[LABEL_COL].unique():
    df_subset = df[df[LABEL_COL] == label]

    if len(df_subset) < majority_size:
        df_resampled = resample(df_subset,
                                replace=True,
                                n_samples=majority_size,
                                random_state=42)
        df_upsampled.append(df_resampled)
        print(f"Upsampled '{label}' from {len(df_subset)} to {majority_size}")
    else:
        df_upsampled.append(df_subset)
        print(f"Class '{label}' is majority/balanced, size {len(df_subset)}")

df_balanced = pd.concat(df_upsampled).sample(frac=1, random_state=42).reset_index(drop=True)
print("\nFinal Balanced Data Shape:", df_balanced.shape)
print("Final Class Counts:\n", df_balanced[LABEL_COL].value_counts())

plt.figure(figsize=(6,4))
sns.barplot(x=df_balanced[LABEL_COL].value_counts().values, y=df_balanced[LABEL_COL].value_counts().index, palette='icefire')
plt.title("Balanced Class Distribution")
plt.show()

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
import numpy as np

le = LabelEncoder()
y_encoded = le.fit_transform(df_balanced[LABEL_COL])
y_categorical = to_categorical(y_encoded)
classes = le.classes_
NUM_CLASSES = len(classes)
print(f"Classes: {classes}, Num Classes: {NUM_CLASSES}")

label_map = {i: c for i, c in enumerate(classes)}
with open(os.path.join(OUT_DIR, "label_map.json"), "w", encoding="utf-8") as f:
    json.dump(label_map, f, indent=4)
print(f"Label map saved to {OUT_DIR}/label_map.json")

X_train_full, X_test, y_train_full, y_test = train_test_split(
    df_balanced[TEXT_COL].values,
    y_categorical,
    test_size=0.2,
    random_state=42,
    stratify=y_encoded
)

stratify_y_train_full = np.argmax(y_train_full, axis=1)

X_train, X_val, y_train, y_val = train_test_split(
    X_train_full,
    y_train_full,
    test_size=0.2,
    random_state=42,
    stratify=stratify_y_train_full
)

print(f"\nTraining set size: {len(X_train)}")
print(f"Validation set size: {len(X_val)}")
print(f"Test set size: {len(X_test)}")

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import tokenizer_from_json

MAX_WORDS = 20000
MAX_LEN = 30

tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<unk>')
tokenizer.fit_on_texts(X_train)

WORD_INDEX = tokenizer.word_index
VOCAB_SIZE = min(len(WORD_INDEX) + 1, MAX_WORDS)
print(f"Vocabulary size: {VOCAB_SIZE}")

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_val_seq = tokenizer.texts_to_sequences(X_val)
X_test_seq = tokenizer.texts_to_sequences(X_test)

X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')
X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding='post', truncating='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')

print(f"\nInput shape (Train): {X_train_pad.shape}")

tokenizer_json = tokenizer.to_json()
with open(os.path.join(OUT_DIR, 'tokenizer.json'), 'w', encoding='utf-8') as f:
    f.write(json.dumps(tokenizer_json, ensure_ascii=False))
print(f"Tokenizer saved to {OUT_DIR}/tokenizer.json")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, SimpleRNN, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

EMBEDDING_DIM = 100
L2_REG = 1e-4

def build_simple_rnn(vocab_size, max_len, num_classes):
    model = Sequential([
        Embedding(vocab_size, EMBEDDING_DIM, input_length=max_len),
        SimpleRNN(64, kernel_regularizer=l2(L2_REG)),
        Dense(64, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ], name="SimpleRNN")
    model.compile(optimizer=Adam(learning_rate=0.001),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

def build_lstm(vocab_size, max_len, num_classes):
    model = Sequential([
        Embedding(vocab_size, EMBEDDING_DIM, input_length=max_len),
        LSTM(128, kernel_regularizer=l2(L2_REG)),
        Dense(64, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ], name="LSTM")
    model.compile(optimizer=Adam(learning_rate=0.001),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

def build_bilstm(vocab_size, max_len, num_classes):
    model = Sequential([
        Embedding(vocab_size, EMBEDDING_DIM, input_length=max_len),
        Bidirectional(LSTM(128, kernel_regularizer=l2(L2_REG))),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ], name="BiLSTM")
    model.compile(optimizer=Adam(learning_rate=0.001),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger
from pathlib import Path

EPOCHS = 6
BATCH_SIZE = 256
PATIENCE = 3

models_to_train = {
    "SimpleRNN": build_simple_rnn(VOCAB_SIZE, MAX_LEN, NUM_CLASSES),
    "LSTM": build_lstm(VOCAB_SIZE, MAX_LEN, NUM_CLASSES),
    "BiLSTM": build_bilstm(VOCAB_SIZE, MAX_LEN, NUM_CLASSES),
}

val_scores = {}
best_overall_acc = -1.0
best_overall_path = ""
best_model_name = ""

for name, model in models_to_train.items():
    print(f"\n==================== TRAINING {name} ====================")

    model_path = os.path.join(OUT_DIR, f"best_{name}.h5")
    log_path = os.path.join(OUT_DIR, "training_logs", f"training_{name}.csv")
    os.makedirs(os.path.dirname(log_path), exist_ok=True)

    callbacks = [
        ModelCheckpoint(model_path, monitor='val_accuracy', save_best_only=True, verbose=1),
        EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True, verbose=1),
        CSVLogger(log_path)
    ]

    start_time = time.time()

    history = model.fit(
        X_train_pad, y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_val_pad, y_val),
        callbacks=callbacks,
        verbose=1
    )

    end_time = time.time()
    train_time_sec = end_time - start_time

    loss, acc = model.evaluate(X_val_pad, y_val, verbose=0)

    val_scores[name] = {
        "val_loss": float(loss),
        "val_acc": float(acc),
        "path": model_path,
        "train_time_sec": train_time_sec
    }

    if acc > best_overall_acc:
        best_overall_acc = acc
        best_overall_path = model_path
        best_model_name = name

shutil.copyfile(best_overall_path, os.path.join(OUT_DIR, "best_model_project.h5"))
print(f"\nBest model ({best_model_name}, Acc: {best_overall_acc:.4f}) saved to {os.path.join(OUT_DIR, 'best_model_project.h5')}")

with open(os.path.join(OUT_DIR, "model_val_scores.json"), "w", encoding="utf-8") as f:
    json.dump(val_scores, f, indent=4)

logs = {}
log_dir = os.path.join(OUT_DIR, "training_logs")

for csvf in os.listdir(log_dir):
    if csvf.endswith(".csv"):
        name = Path(csvf).stem.replace("training_", "")
        df_log = pd.read_csv(os.path.join(log_dir, csvf))
        logs[name] = df_log

for name, df_log in logs.items():
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(df_log['loss'], label='Train Loss')
    if 'val_loss' in df_log.columns:
        plt.plot(df_log['val_loss'], label='Validation Loss')
    plt.title(f"{name} Model Loss")
    plt.xlabel("Epoch")
    plt.legend()

    plt.subplot(1, 2, 2)
    col = 'accuracy' if 'accuracy' in df_log.columns else 'acc'
    valcol = 'val_accuracy' if 'val_accuracy' in df_log.columns else 'val_acc'

    plt.plot(df_log[col], label='Train Accuracy')
    if valcol in df_log.columns:
        plt.plot(df_log[valcol], label='Validation Accuracy')

    plt.title(f"{name} Model Accuracy")
    plt.xlabel("Epoch")
    plt.legend()
    plt.tight_layout()
    plt.show()

with open(os.path.join(OUT_DIR, "model_val_scores.json"), "r", encoding="utf-8") as f:
    val_scores = json.load(f)

def format_time(seconds):
    hours, remainder = divmod(seconds, 3600)
    minutes, seconds = divmod(remainder, 60)
    return f"{int(hours)}h {int(minutes)}m {int(seconds)}s"

results_data = []
best_acc = max(score['val_acc'] for score in val_scores.values())

for model_name, metrics in val_scores.items():
    results_data.append({
        "Model Architecture": model_name,
        "Validation Accuracy": f"{metrics['val_acc']:.4f}",
        "Validation Loss": f"{metrics['val_loss']:.4f}",
        "Training Time": format_time(metrics['train_time_sec']),
        "Status": "BEST (Final Selection)" if metrics['val_acc'] == best_acc else "Tested"
    })

results_df = pd.DataFrame(results_data)
print("\n=== MODEL PERFORMANCE SUMMARY ===")
display(results_df)

val_scores_df_viz = pd.DataFrame(val_scores).T.reset_index()
val_scores_df_viz.rename(columns={'index': 'Model Architecture'}, inplace=True)

plt.figure(figsize=(9,6))
sns.barplot(x='Model Architecture', y='val_acc', data=val_scores_df_viz, palette='Spectral')
plt.title("Model Architecture vs Validation Accuracy")
plt.ylabel("Validation Accuracy (0.0 to 1.0)")
plt.ylim(0, 1.0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

from tensorflow.keras.models import load_model
from sklearn.metrics import classification_report

final_model = load_model(os.path.join(OUT_DIR, "best_model_project.h5"))
print(f"Loaded final model: {final_model.name}")

print("\nEvaluating on Test Set...")
test_loss, test_acc = final_model.evaluate(X_test_pad, y_test, verbose=1)
print(f"Test Set Accuracy: {test_acc:.4f}, Test Set Loss: {test_loss:.4f}")

y_pred_probs = final_model.predict(X_test_pad)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_test, axis=1)

report = classification_report(y_true, y_pred, target_names=classes, output_dict=True)
report_str = classification_report(y_true, y_pred, target_names=classes)

print("\n=== CLASSIFICATION REPORT ===")
print(report_str)

with open(os.path.join(OUT_DIR, "classification_report.json"), "w", encoding="utf-8") as f:
    json.dump(report, f, indent=4)
print(f"Classification report saved to {OUT_DIR}/classification_report.json")

def predict_sentiment(text_input):
    if 'tokenizer' not in globals():
        with open(os.path.join(OUT_DIR, 'tokenizer.json'), 'r', encoding='utf-8') as f:
            tokenizer_json = f.read()
            global tokenizer
            tokenizer = tokenizer_from_json(tokenizer_json)

    if 'label_map' not in globals():
        with open(os.path.join(OUT_DIR, "label_map.json"), "r", encoding="utf-8") as f:
            global label_map
            label_map = {int(k): v for k, v in json.load(f).items()}

    seq = tokenizer.texts_to_sequences([text_input])
    padded_seq = pad_sequences(seq, maxlen=MAX_LEN, padding='post', truncating='post')

    probabilities = final_model.predict(padded_seq, verbose=0)[0]
    predicted_index = np.argmax(probabilities)
    predicted_label = label_map[predicted_index]

    prob_dict = {label_map[i]: float(p) for i, p in enumerate(probabilities)}

    return predicted_label, prob_dict

test_input = "The instructor's explanation was very clear and helpful."
pred_label, probs = predict_sentiment(test_input)
print("\n--- Quick Prediction Test ---")
print(f"Text: {test_input}")
print(f"Predicted Label: {pred_label}")
print(f"Probabilities: {probs}")

demo_samples = [
    "The course content was fantastic and the instructor was engaging.",
    "I hated this class, total waste of time, I learned nothing useful.",
    "The material was okay, but the labs were confusing.",
    "Absolutely brilliant teaching method and extremely clear content.",
    "The system is very slow and not user-friendly at all."
]

demo_data = []

print("\nGenerating Demo Input/Output file...")
for text in demo_samples:
    try:
        pred_label, conf_dict = predict_sentiment(text)
        confidence_score = conf_dict.get(pred_label, "N/A")

        demo_data.append({
            "Input_Text": text,
            "Predicted_Label": pred_label,
            "Confidence": f"{confidence_score:.4f}"
        })
    except Exception as e:
        print(f"Error during demo prediction for text '{text}': {e}")
        break

if demo_data:
    demo_df = pd.DataFrame(demo_data)
    demo_file_path = os.path.join(OUT_DIR, "demo_input_output.tsv")
    demo_df.to_csv(demo_file_path, sep='\t', index=False)

    print(f"Demo file saved to: {demo_file_path}")
    print("\nVerification Samples Table:")
    display(demo_df)

print("Generating requirements.txt for reproducibility...")
!pip freeze > {os.path.join(OUT_DIR, "requirements.txt")}

checklist_content = """
NNFLL Final Project Submission Checklist:

1. best_model_project.h5 - The weights of the best performing model.
2. tokenizer.json - The tokenizer fitted on the training data.
3. label_map.json - Mapping of integers to class names.
4. classification_report.json - Detailed metrics of the final model on the test set.
5. demo_input_output.tsv - 5 sample inputs with model predictions.
6. model_val_scores.json - JSON containing validation scores and training times.
7. training_logs/*.csv - Training history logs for all models.
8. requirements.txt - Full list of Python libraries.
"""
with open(os.path.join(OUT_DIR, "SUBMISSION_CHECKLIST.txt"), "w", encoding="utf-8") as f:
    f.write(checklist_content.strip())

GROUP_IDS = "ID1_ID2_ID3"
zip_filename = f"CODE_{GROUP_IDS}"
zip_path = f"/content/{zip_filename}"

if os.path.exists(zip_path + ".zip"):
    os.remove(zip_path + ".zip")

shutil.make_archive(zip_path, 'zip', OUT_DIR)
print(f"\nSUCCESS: Project artifacts zipped to {zip_path}.zip")